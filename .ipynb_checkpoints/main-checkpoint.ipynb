{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "import torchvision\n",
    "import random\n",
    "\n",
    "batch_size_train = 100\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "\n",
    "\n",
    "### important hyperparameters\n",
    "num_of_workers=20\n",
    "num_of_mal_workers=0\n",
    "n_iter=50\n",
    "n_epochs=5\n",
    "poison_starts_at_iter=n_iter\n",
    "inertia=0.1\n",
    "momentum=0.1\n",
    "attack_type='label_flip'\n",
    "scale_up=False\n",
    "minimizeDist=False\n",
    "\n",
    "iid = False\n",
    "num_of_distributions = int(num_of_workers/4)\n",
    "num_of_workers_in_distribs = num_of_workers * np.random.dirichlet(np.array(num_of_distributions * [3.0]))\n",
    "num_of_workers_in_distribs = [int(val) for val in num_of_workers_in_distribs]\n",
    "while 0 in num_of_workers_in_distribs:\n",
    "    num_of_workers_in_distribs.remove(0)\n",
    "num_of_workers_in_distribs.append(num_of_workers-sum(num_of_workers_in_distribs))\n",
    "print(num_of_workers_in_distribs, sum(num_of_workers_in_distribs))\n",
    "num_of_distributions = len(num_of_workers_in_distribs)\n",
    "copylist = []\n",
    "for i in range(len(num_of_workers_in_distribs)):\n",
    "    copylist += num_of_workers_in_distribs[i]*[i]\n",
    "random.shuffle(copylist)\n",
    "print(copylist)\n",
    "\n",
    "label_skew_ratios=[]\n",
    "\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ae995",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "#test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c57ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "dataPath = ''\n",
    "\n",
    "import random\n",
    "\n",
    "train_loaders=[]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "    ### if dataset is mnist\n",
    "    transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    ### if dataset is cifar\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST('', train=True, download=True,\n",
    "                               transform=transform)\n",
    "test_dataset = datasets.FashionMNIST('', train=False, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "all_range = list(range(len(train_dataset)))\n",
    "random.shuffle(all_range)\n",
    "\n",
    "def get_train_iid(all_range, model_no, iter_no):\n",
    "    \"\"\"\n",
    "    This method equally splits the dataset.\n",
    "    :param params:\n",
    "    :param all_range:\n",
    "    :param model_no:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    data_len_for_iter = int(len(train_dataset) / n_iter)\n",
    "    data_len = int(data_len_for_iter/num_of_workers)\n",
    "    sub_indices_for_iter = all_range[iter_no * data_len_for_iter: (iter_no + 1) * data_len_for_iter]\n",
    "    sub_indices = sub_indices_for_iter[model_no * data_len: (model_no + 1) * data_len ]\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=batch_size_train,\n",
    "                                        sampler=torch.utils.data.sampler.SubsetRandomSampler(sub_indices)\n",
    "                                        )\n",
    "    return train_loader\n",
    "\n",
    "def get_train_noniid(indices):\n",
    "    \"\"\"\n",
    "    This method is used along with Dirichlet distribution\n",
    "    :param params:\n",
    "    :param indices:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=int(len(train_dataset)/num_of_workers),\n",
    "                                        sampler=torch.utils.data.sampler.SubsetRandomSampler(\n",
    "                                            indices))\n",
    "    return train_loader\n",
    "\n",
    "def poison_test_dataset(test_dataset, batch_size):\n",
    "    logger.info('get poison test loader')\n",
    "    # delete the test data with target label\n",
    "    test_classes = {}\n",
    "    for ind, x in enumerate(test_dataset):\n",
    "        _, label = x\n",
    "        if label in test_classes:\n",
    "            test_classes[label].append(ind)\n",
    "        else:\n",
    "            test_classes[label] = [ind]\n",
    "\n",
    "    range_no_id = list(range(0, len(test_dataset)))\n",
    "    for image_ind in test_classes[poison_dict['poison_label_swap']]:\n",
    "        if image_ind in range_no_id:\n",
    "            range_no_id.remove(image_ind)\n",
    "    poison_label_inds = test_classes[poison_dict['poison_label_swap']]\n",
    "\n",
    "    return torch.utils.data.DataLoader(test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        sampler=torch.utils.data.sampler.SubsetRandomSampler(\n",
    "                            range_no_id)), \\\n",
    "            torch.utils.data.DataLoader(test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        sampler=torch.utils.data.sampler.SubsetRandomSampler(\n",
    "                                            poison_label_inds))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afeb8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sample_dirichlet_train_data(no_participants=num_of_workers, dataset=train_dataset, alpha=0.9, copylist=np.arange(num_of_workers)):\n",
    "    \"\"\"\n",
    "        Input: Number of participants and alpha (param for distribution)\n",
    "        Output: A list of indices denoting data in CIFAR training set.\n",
    "        Requires: dataset_classes, a preprocessed class-indice dictionary.\n",
    "        Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "        dirichlet distribution to sample number of images in each class.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_classes = {}\n",
    "    for ind, x in enumerate(dataset):\n",
    "        _, label = x\n",
    "        #if ind in self.params['poison_images'] or ind in self.params['poison_images_test']:\n",
    "        #    continue\n",
    "        if label in dataset_classes:\n",
    "            dataset_classes[label].append(ind)\n",
    "        else:\n",
    "            dataset_classes[label] = [ind]\n",
    "    class_size = len(dataset_classes[0])\n",
    "    per_participant_list = defaultdict(list)\n",
    "    no_classes = len(dataset_classes.keys())\n",
    "\n",
    "    for n in range(no_classes):\n",
    "        random.shuffle(dataset_classes[n])\n",
    "        num_of_non_iid_participants = len(np.unique(copylist))\n",
    "        sampled_probabilities = np.random.dirichlet(\n",
    "            np.array(num_of_non_iid_participants * [alpha]))\n",
    "        new_list = []\n",
    "        for ip in copylist:\n",
    "            #new_list.append(np.random.normal(loc=sampled_probabilities[ip], scale=0.005))\n",
    "            new_list.append(sampled_probabilities[ip])\n",
    "        sampled_probabilities = class_size * np.array(new_list)/np.sum(np.array(new_list))\n",
    "        sigmas = 0.0 * sampled_probabilities\n",
    "        sampled_probabilities = np.random.normal(sampled_probabilities, scale=sigmas)\n",
    "        print(sampled_probabilities)\n",
    "        label_skew_ratios.append(sampled_probabilities)\n",
    "        for user in range(no_participants):\n",
    "            no_imgs = int(round(sampled_probabilities[user]))\n",
    "            sampled_list = dataset_classes[n][:min(len(dataset_classes[n]), no_imgs)]\n",
    "            per_participant_list[user].extend(sampled_list)\n",
    "            dataset_classes[n] = dataset_classes[n][min(len(dataset_classes[n]), no_imgs):]\n",
    "\n",
    "    return per_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967afaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if iid:\n",
    "    train_loaders=[]\n",
    "    for i in range(n_iter):\n",
    "        train_loaders.append([(i, pos, get_train_iid(all_range, pos, i))\n",
    "                                for pos in range(num_of_workers)])\n",
    "else:\n",
    "    indices_per_participant = sample_dirichlet_train_data(\n",
    "        num_of_workers,\n",
    "        #dataset= torch.utils.data.Subset(train_dataset, list(range(240))),\n",
    "        alpha=0.95,\n",
    "        copylist=copylist)\n",
    "    train_loaders = [(-1, pos, get_train_noniid(indices)) for pos, indices in\n",
    "                    indices_per_participant.items()]\n",
    "    train_loaders = n_iter * [train_loaders]\n",
    "\n",
    "print(train_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = CustomFL(n_iter=n_iter, n_epochs=n_epochs, poison_starts_at_iter=poison_starts_at_iter, num_of_benign_nets=num_of_workers-num_of_mal_workers, num_of_mal_nets=num_of_mal_workers, \n",
    "              inertia=inertia, momentum=momentum,\n",
    "              attack_type=attack_type, scale_up=scale_up, minimizeDist=minimizeDist\n",
    ")\n",
    "fl.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3113d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'file_1_dec27_all_benign_iters_50_epoch_{n_epochs}.txt', 'wb') as f:\n",
    "    pickle.dump(fl.debug_log, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
